apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: {{ .Release.Name }}-spark
spec:
  type: Scala
  mode: cluster
  image: "{{ tpl .Values.global.imageUrls.firewallImageUrl . }}:{{ .Values.firewallImage.image.firewallImageTag }}"
  imagePullPolicy: IfNotPresent
  imagePullSecrets:
    - regcred
  restartPolicy:
    type: Always
    onFailureRetries: 10
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 10
    onSubmissionFailureRetryInterval: 10
  mainClass: io.qualytics.firewall.SparkMothership
  mainApplicationFile: "local:///opt/spark/jars/firewall-core.jar"
  sparkVersion: {{ .Values.firewall.sparkVersion }}
  sparkConf:
    spark.kubernetes.memoryOverheadFactor: "0.2"
    spark.eventLog.enabled: {{ .Values.firewall.eventLog.enable | quote }}
    spark.kubernetes.submission.connectionTimeout: "480000"
    spark.kubernetes.submission.requestTimeout: "480000"
    spark.kubernetes.driver.connectionTimeout: "480000"
    spark.kubernetes.driver.requestTimeout: "480000"
  volumes:
    - name: "spark-local-dir-driver"
    {{- if ( eq .Values.global.platform "aws" ) }}
      hostPath:
        path: "/local-1/spark-local-dir"
    {{- else if ( eq .Values.global.platform "gcp" ) }}
      hostPath:
        path: "/var/spark-local-dir"
    {{- else }}
      emptyDir: {}
    {{- end }}
    - name: "spark-local-dir-executor"
    {{- if ( eq .Values.global.platform "aws" ) }}
      hostPath:
        path: "/local-1/spark-local-dir"
    {{- else if ( eq .Values.global.platform "gcp" ) }}
      hostPath:
        path: "/var/spark-local-dir"
    {{- else }}
      emptyDir: {}
    {{- end }}
  driver:
{{- $resources := .Values.firewall -}}
    {{- with $resources.driver }}
    nodeSelector:
      sparkNodes: "true"
    cores: {{ .cores }}
    coreLimit: {{ .coreLimit }}
    memory: {{ .memory }}
    {{- end }}
    javaOptions:
      "-Divy.cache.dir=/tmp
       -Divy.home=/tmp
       -Dlog4j.configuration=file:/opt/spark/log4j.properties
       -Dconfig.resource=prod.conf
       -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:InitiatingHeapOccupancyPercent=35"
#--------------------------------------------------------------------------------
# Environments that require a proxy (Http, Socks, etc..) for egress
#--------------------------------------------------------------------------------
# The simplest solution is to solve for this outside of Qualytics. For example, using
#   an istio egress configured for your proxy or another similar gateway device.
#
# However, if proxy-aware applications are required, the javaOptions string above can be
#  updated to hold the relevant proxy information according to the JRE v17 spec:
#    https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/net/doc-files/net-properties.html
#
#  If additionally, authentication is required for an http(s) proxy, use the following additional
#   properties not documented on the page linked above:
#  -Dhttps.proxyUser=your_username
#  -Dhttps.proxyPassword=your_password
#  -Dhttp.proxyUser=your_username
#  -Dhttp.proxyPassword=your_password
#
    env:
      - name: MOTHERSHIP_SURVEILLANCE_HUB_URL
        value: {{ printf "http://%s-api-service:%s/api" .Release.Name .Values.hub.ingress.servicePort }}
      - name: MOTHERSHIP_AUTH0_DOMAIN
        value: "https://{{ .Values.secrets.auth0.auth0_domain }}"
      - name: MOTHERSHIP_AUTH0_AUDIENCE
        valueFrom:
          secretKeyRef:
            name: qualytics-creds
            key: auth0_audience
      - name: MOTHERSHIP_AUTH0_CLIENT_ID
        valueFrom:
          secretKeyRef:
            name: qualytics-creds
            key: auth0_client_id
      - name: MOTHERSHIP_AUTH0_CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            name: qualytics-creds
            key: auth0_client_secret
      - name: MOTHERSHIP_RABBIT_HOST
        value: "{{ .Release.Name }}-rabbitmq"
      - name: MOTHERSHIP_RABBIT_PASS
        valueFrom:
          secretKeyRef:
            name: qualytics-creds
            key: rabbitmq_password
      - name: MOTHERSHIP_RABBIT_USER
        value: "user"
      - name: MOTHERSHIP_USE_CACHE
        value: {{ .Values.firewall.useCache | quote }}
      - name: MOTHERSHIP_NUM_REPARTITION
        value: {{ .Values.firewall.numRepartition | quote }}
      - name: MOTHERSHIP_PARTITION_SIZE
        value: {{ .Values.firewall.partitionSize | int | quote }}
    {{- if gt ( .Values.firewall.threadPoolParallelism | int ) 0 }}
      - name: MOTHERSHIP_THREAD_POOL_PARALLELISM
        value: {{ .Values.firewall.threadPoolParallelism | quote }}
    {{- end }}
    labels:
      version: {{ .Values.firewall.sparkVersion }}
    serviceAccount: {{ .Release.Name }}-spark
    volumeMounts:
      - name: "spark-local-dir-driver"
        mountPath: "/tmp/spark-local-dir"
    securityContext:
      runAsUser: 185
    initContainers:
    - name: init-spark-local-dir
      image: busybox
      command: ['sh', '-c', 'chmod 777 /tmp/spark-local-dir']
      securityContext:
        runAsUser: 0
      volumeMounts:
      - name: "spark-local-dir-driver"
        mountPath: "/tmp/spark-local-dir"
  dynamicAllocation:
    {{- with $resources.dynamicAllocation }}
    enabled: true
    initialExecutors: {{ .initialExecutors }}
    minExecutors: {{ .minExecutors }}
    maxExecutors: {{ .maxExecutors }}
    {{- end }}
  executor:
    {{- with $resources.executor }}
    instances: {{ .instances }}
    cores: {{ .cores }}
    coreLimit: {{ .coreLimit }}
    memory: {{ .memory }}
    {{- end }}
    javaOptions:
      "-Dlog4j.configuration=file:/opt/spark/log4j.properties
       -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:InitiatingHeapOccupancyPercent=35"
    labels:
      version: {{ .Values.firewall.sparkVersion }}
    nodeSelector:
      sparkNodes: "true"
    volumeMounts:
      - name: "spark-local-dir-executor"
        mountPath: "/tmp/spark-local-dir"
    securityContext:
      runAsUser: 185
    initContainers:
    - name: init-spark-local-dir
      image: busybox
      command: ['sh', '-c', 'chmod 777 /tmp/spark-local-dir']
      securityContext:
        runAsUser: 0
      volumeMounts:
      - name: "spark-local-dir-executor"
        mountPath: "/tmp/spark-local-dir"
